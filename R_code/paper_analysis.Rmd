---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
library(scales)
library(tidyverse)
library(caret)
library(ggpubr)
library(pROC)
```
```{r}
validation_data_prefix = "/Users/noa/Workspace/bootstrap_results/validation_data"
fasttree_validation_data = read_tsv(paste(validation_data_prefix,"/simulations_df_fasttree.tsv", sep = ""))
raxml_validation_data = read_tsv(paste(validation_data_prefix,"/simulations_df_raxml.tsv", sep = ""))
iqtree_validation_data = read_tsv(paste(validation_data_prefix,"/simulations_df_iqtree.tsv", sep = ""))
```



```{r}
prefix = "/Users/noa/Workspace/bootstrap_results/remote_results/"


fasttree_all_models_performance = read_tsv(paste(prefix,"ML_results/fasttree/all_models_performance.tsv", sep = ""))
fasttree_final_models_performance = read_tsv(paste(prefix,"ML_results/fasttree/final_model_performance.tsv", sep = ""))
fasttree_group_performance = read_tsv(paste(prefix,"ML_results/fasttree/groups_performance.tsv", sep = ""))
fasttree_vi =  read_tsv(paste(prefix,"ML_results/fasttree/model_standard_vi.tsv", sep = ""))
fasttree_enriched_test = read_tsv(paste(prefix,"ML_results/fasttree/test_model_standard_enriched.tsv", sep = ""))
fasttree_validation = read_tsv(paste(prefix,"validation_data/simulations_df_fasttree.tsv", sep = ""))
#####
iqtree_all_models_performance = read_tsv(paste(prefix,"ML_results/iqtree/all_models_performance.tsv", sep = ""))
iqtree_final_models_performance = read_tsv(paste(prefix,"/ML_resultsiqtree/final_model_performance.tsv", sep = ""))
iqtree_group_performance = read_tsv(paste(prefix,"ML_results/iqtree/groups_performance.tsv", sep = ""))
iqtree_vi =  read_tsv(paste(prefix,"ML_results/iqtree/model_standard_vi.tsv", sep = ""))
iqtree_enriched_test = read_tsv(paste(prefix,"ML_results/iqtree/test_model_standard_enriched.tsv", sep = ""))
iqtree_validation = read_tsv(paste(prefix,"validation_data/simulations_df_iqtree.tsv", sep = ""))
#####
raxml_all_models_performance = read_tsv(paste(prefix,"ML_results/raxml/all_models_performance.tsv", sep = ""))
raxml_final_models_performance = read_tsv(paste(prefix,"ML_results/raxml/final_model_performance.tsv", sep = ""))
raxml_group_performance = read_tsv(paste(prefix,"ML_results/raxml/groups_performance.tsv", sep = ""))
raxml_vi =  read_tsv(paste(prefix,"ML_results/raxml/model_standard_vi.tsv", sep = ""))
raxml_enriched_test = read_tsv(paste(prefix,"ML_results/raxml/test_model_standard_enriched.tsv", sep = ""))
raxml_validation = read_tsv(paste(prefix,"validation_data/simulations_df_raxml.tsv", sep = ""))


```

Choose specific program
```{r}
program = 'raxml'
if (program=='fasttree') {
    all_models_performance = fasttree_all_models_performance
    final_models_performance = fasttree_final_models_performance %>% filter(analysis_type!='feature_standard_fasttree_boot_support')
    group_performance = fasttree_group_performance
    vi = fasttree_vi
    enriched_test = fasttree_enriched_test
    validation_data = fasttree_validation_data
    
} else if (program=='iqtree') {
    all_models_performance = iqtree_all_models_performance
    final_models_performance = iqtree_final_models_performance
    group_performance = iqtree_group_performance
    vi = fasttree_vi
    enriched_test = iqtree_enriched_test
    validation_data = iqtree_validation_data
} else if (program=='raxml') {
    all_models_performance = raxml_all_models_performance
    final_models_performance = raxml_final_models_performance
    group_performance = raxml_group_performance
    vi = fasttree_vi
    enriched_test = raxml_enriched_test
    validation_data = raxml_validation_data
}
```


General performance
```{r}

out_path <-paste(prefix,"/",program,"/vi_text",".tsv", sep = "")

paper_vi<-final_models_performance  %>% filter ((dataset=='test'|dataset=='train') , metric_type=='all_data') %>% dplyr::select (dataset,analysis_type,name,mcc,balanced_accuracy_score,precision, recall, AUC,average_precision)  %>% arrange(dataset)
write_tsv(paper_vi,out_path)

paper_vi

```




Variable importance 
```{r}
vi %>% arrange (-`Gini-importance`)
```



Confusion matrix

```{r}
ggplotConfusionMatrix <- function(m){
  mytitle <- ""#paste("Accuracy", percent_format()(m$overall[1]))
                  # "Kappa", percent_format()(m$overall[2]))
  p <-
    ggplot(data = as.data.frame(m$table) ,
           aes(x = Reference, y = Prediction)) +
    geom_tile(aes(fill = log(Freq)), colour = "white") +
    scale_fill_gradient(low = "white", high = "steelblue") +
    geom_text(aes(x = Reference, y = Prediction, label = scales::comma(Freq))) +
    theme(legend.position = "none") +
    ggtitle(mytitle)+theme(text = element_text(size = 13))
  return(p)
}
conf_data<-enriched_test %>% dplyr::select (prob_predictions, true_label) %>% mutate (pred_status = prob_predictions>0.5) 

obs<- factor(conf_data %>% pull (true_label))
pred<- factor(conf_data %>% pull (pred_status))


confusion_plot<-ggplotConfusionMatrix(confusionMatrix(data = pred, reference= obs))+theme(legend.position = "none")

```
Prediction metrics


```{r}
library(predtools)
library(MASS)
library(caret)

error_vs_size
error_vs_size<- all_models_performance %>% filter (metric_type=='all_data', dataset =='test')
error_vs_size_plt = error_vs_size %>% mutate(n_MSAs = sample_frac*2007) %>% ggplot(aes(x=n_MSAs, y = mcc))+geom_line()+ expand_limits(y = 0)+xlab("Number of MSAs")+ theme(text = element_text(size = 13))+ylab("MCC")+geom_point()

rocobj <- roc(enriched_test$true_label, enriched_test$prob_predictions)
auc <- round(auc(enriched_test$true_label, enriched_test$prob_predictions),3)

global_max_auc_plt<-ggroc(rocobj,colour = 'steelblue', size = 2)+
  ggtitle(paste0('AUC = ', auc))+theme(text = element_text(size = 13))+xlab("Specificity")




ggarrange(global_max_auc_plt,confusion_plot,error_vs_size_plt, labels = c("A","B","C"),align = "h", nrow = 2, ncol = 2 )

```
Group performance

```{r}
test_group_performance<- group_performance %>% filter (dataset=='test')

Pypythia_error<-test_group_performance %>% filter (grouping_col_name =='msa_difficulty_group') %>% ggplot(aes(y=grouping_col, x=Balanced_Accuracy))+geom_col()+ expand_limits(y = c(0.5,1))+ylab('MSA\ndifficulty')+ xlab('Balanced\naccuracy')+theme(text = element_text(size = 11))

n_seq_error<-test_group_performance %>% filter (grouping_col_name =='n_seq_group')  %>% ggplot(aes(y=grouping_col, x=Balanced_Accuracy))+geom_col()+ expand_limits(y = c(0.5,1))+ylab('Number\nof sequences')+xlab('Balanced\naccuracy')+theme(text = element_text(size = 11))

#>%  mutate(grouping_col = factor(grouping_col, levels = c('(99.999, 219.0]','(219.0, 546.0]','(546.0, 1519.0]','(1519.0, 16115.0]')))

n_loci_error<-test_group_performance %>% filter (grouping_col_name =='feature_msa_n_loci') %>% ggplot(aes(y=grouping_col, x=Balanced_Accuracy))+geom_col()+ expand_limits(y = c(0.5,1))+ylab('Number\nof positions')+xlab('Balanced\naccuracy')+theme(text = element_text(size = 11))

Pypythia_scatter<-enriched_test %>% dplyr::select (msa_path,prob_predictions,  feature_msa_pypythia_msa_difficulty) %>% group_by(msa_path,feature_msa_pypythia_msa_difficulty) %>% summarise(mean_prob = mean(prob_predictions)) %>% ggplot(aes(x=feature_msa_pypythia_msa_difficulty, y = mean_prob))+geom_point(size = 0.7)+xlab("MSA\ndifficulty")+ylab("Predicted\nprobability")+theme(text = element_text(size = 11))

seq_scatter<-enriched_test %>% dplyr::select (msa_path,prob_predictions, feature_msa_n_seq, feature_msa_pypythia_msa_difficulty) %>% group_by(msa_path,feature_msa_n_seq) %>% summarise(mean_prob = mean(prob_predictions)) %>% ggplot(aes(x=feature_msa_n_seq, y = mean_prob))+geom_point(size = 0.7)+xlab("Number of\nsequences")+ylab("Predicted\nprobability")+theme(text = element_text(size = 11))

n_loci_scatter<-enriched_test %>% dplyr::select (msa_path,prob_predictions, feature_msa_n_loci) %>% group_by(msa_path,feature_msa_n_loci) %>% summarise(mean_prob = mean(prob_predictions)) %>% ggplot(aes(x=feature_msa_n_loci, y = mean_prob))+geom_point(size = 0.7)+xlab("Number of\n positions")+ylab("Predicted\nprobability")+theme(text = element_text(size = 11))

ggarrange(seq_scatter,Pypythia_scatter,n_loci_scatter, n_seq_error,Pypythia_error, n_loci_error,labels = c("A","C","E","B","D","F"),align = "h", nrow = 3, ncol = 3, legend = "bottom",vjust= 1)
#error_vs_group_5_5_including_plt


```

```{r}
cor.test( enriched_test$feature_msa_pypythia_msa_difficulty,  enriched_test$prob_predictions)
cor.test( enriched_test$feature_msa_n_seq, enriched_test$prob_predictions)
cor.test( enriched_test$feature_msa_n_loci, enriched_test$prob_predictions)
```



Validation_data
```{r}
raxml_data = read_tsv("/Users/noa/Workspace/simulations_df_raxml.tsv")

```


```{r}
running_time_data<- enriched_test%>% dplyr::select (boot_run_time,extraction_running_time,nni_ll_eval_running_time) %>% mutate (diff1 = boot_run_time/extraction_running_time, diff2 = boot_run_time/(extraction_running_time+nni_ll_eval_running_time
)) 
summary(running_time_data %>% pull (diff1))
summary(running_time_data %>% pull (diff2))
summary(running_time_data %>% pull (extraction_running_time))
summary(running_time_data %>% pull (nni_ll_eval_running_time))
#enriched_test %>% dplyr::select (boot_run_time, extraction_runni
```



```{r}


running_time_data<- enriched_test%>% dplyr::select (program_boot_run_time,extraction_running_time,nni_ll_eval_running_time) %>% mutate (diff1 = program_boot_run_time/extraction_running_time, diff2 = program_boot_run_time/(extraction_running_time+nni_ll_eval_running_time
)) 
summary(running_time_data %>% pull (diff1))
summary(running_time_data %>% pull (diff2))
summary(running_time_data %>% pull (extraction_running_time))
summary(running_time_data %>% pull (nni_ll_eval_running_time))
summary(running_time_data %>% pull (program_boot_run_time))
#enriched_test %>% dplyr::select (boot_run_time, extraction_running_time, nni_ll_eval_running_time)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

